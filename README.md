# ğŸ§  TwinSqueeze: Contrastive Compression with NEFTune Regularization

**TwinSqueeze** is a Siamese network designed to compress high-dimensional sentence embeddings (e.g., 384-dim from MiniLM) into a smaller, task-optimized vector (e.g., 32-dim) while preserving cosine similarity. This project also systematically studies **NEFTune**, a simple noise-based regularization strategy, by analyzing its effect on generalization and ranking quality.

---

## ğŸ¯ Goals

1. **Compress Embedding Vectors**  
   Reduce dimensionality (e.g. 384 â†’ 32) to improve:
   - **Latency in RAG pipelines**
   - **Memory usage in embedding storage**
   - **Speed of ANN vector search** for tools like FAISS, qdrant, or custom indexers

2. **Task-Aligned Similarity**  
   Along with preserving raw sentence similarity, TwinSqueeze is trained to:
   - Compress embeddings to **reflect domain/task-specific cosine similarity**
   - Optimize for **ranking alignment** (Spearman) â€” crucial for information retrieval
   - Modify similarity depending on your use case, improving results for custom RAG needs

3. **Plug-and-Play for Custom RAG Systems**  
   TwinSqueeze can be integrated into:
   - ğŸ”¹ **Voyager-like local memory agents** to compress thought/action logs
   - ğŸ”¹ **Local blog/document retrievers** to reduce storage and maintain performance
   - ğŸ”¹ Any **custom RAG pipeline** to:
     - Decrease search/query cost
     - Improve ranking robustness (via NEFTune)
     - Handle noisy input or noisy memory gracefully

> ğŸ“Œ If you're building your own agent memory or domain-specific retriever, TwinSqueeze helps scale similarity logic without depending on external APIs.

---

## ğŸš€ What This Project Does

- âœ… Compresses sentence embeddings using a 3-layer MLP
- âœ… Trains on STS-B sentence similarity scores using cosine + MSE
- âœ… Applies **NEFTune** by injecting noise into input embeddings
- âœ… Evaluates models using:
  - MSE / MAE (point error)
  - Pearson / Spearman (ranking)
- âœ… Visualizes:
  - Training loss curves
  - Per-sample prediction accuracy
  - Loss distributions (paper-style)

---

## ğŸ“¦ Project Structure

```
twin_squeeze_pipeline/
â”‚
â”œâ”€â”€ model.py              # Siamese compression + cosine similarity
â”œâ”€â”€ trainer.py            # Training loop with loss logging
â”œâ”€â”€ logger.py             # Result saving, chart export, CSV writing
â”œâ”€â”€ dataloader.py         # Dataset loading + NEFTune noise injection
â”œâ”€â”€ test.py               # Evaluation metrics + prediction plots
â”œâ”€â”€ neftune_utils.py      # (Optional) NEFTTune loss analysis tools
â”‚
â”œâ”€â”€ models/               # Trained .pt model weights
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ benchmark_charts/ # All visualizations and comparisons
â”‚   â””â”€â”€ metrics_stsb.csv  # Final metric scores across runs
```

---

## ğŸ§ª Experimental Setup

### Dataset
- **STS-B** from GLUE benchmark
- Scores normalized to [0, 1] and scaled using tanh-like function

### Embeddings
- Sentence embeddings from `all-MiniLM-L6-v2` (384-dim)
- Noise added **before compression** as per NEFTune formula:
```
embedding_noised = embedding + Uniform(-1,1) * (alpha / sqrt(dim))
```

### Model Architecture
- Twin input â†’ shared MLP compression block:
```
Linear(384 â†’ 128) â†’ ReLU â†’
Linear(128 â†’ 64) â†’ ReLU â†’
Linear(64 â†’ compressed_dim)
```
- Cosine similarity computed after normalization
- Trained with MSE loss

---

## ğŸ§© NEFTune Ablation

We tested the effect of NEFTune on compressed representations using various `alpha` values:

| Model               | Î±    | MSE     | MAE     | Pearson | Spearman |
|--------------------|------|---------|---------|---------|----------|
| **Baseline**        | 0.0  | 0.0720  | 0.2034  | 0.7310  | 0.7750   |
| twin_cd32_alpha0.5  | 0.5  | 0.0683  | 0.1983  | 0.7740  | **0.8145** |
| twin_cd32_alpha0.75 | 0.75 | 0.0720  | 0.2009  | 0.7738  | **0.8185** |
| twin_cd32_alpha1.0  | 1.0  | 0.0823  | 0.2151  | 0.7481  | 0.7984   |
| twin_cd32_alpha3.0  | 3.0  | 0.1538  | 0.2987  | 0.6475  | 0.7622   |

### ğŸ† Key Insights

- `Î± = 0.5â€“0.75` improves **Spearman correlation**
- `Î± = 3.0` hurts both generalization and prediction alignment
- NEFTune introduces higher training loss (as expected), but better test generalization

---

## ğŸ“Š Visualizations

### âœ… Training Loss Comparison (all alphas)
Shows how training loss evolves across epochs for each NEFTune `Î±` value. Baseline converges faster, but lower noise (Î± = 0.5, 0.75) still tracks well while improving generalization.

![Training Loss Comparison](results/loss_comparison.png)

---

### âœ… Per-Model Cosine Prediction vs Ground Truth
Visualizes how predicted cosine similarity aligns with ground truth for the first 50 validation samples. Smoother alignment reflects better ranking behavior.

- **Î± = 0.5**
  
  ![Alpha 0.5 Cosine Match](results/benchmark_charts/twin_cd32_alpha0.5_vs_truth.png)

- **Î± = 0.75**
  
  ![Alpha 0.75 Cosine Match](results/benchmark_charts/twin_cd32_alpha0.75_vs_truth.png)

- **Î± = 1.0**
  
  ![Alpha 1.0 Cosine Match](results/benchmark_charts/twin_cd32_alpha1.0_vs_truth.png)

- **Î± = 3.0**
  
  ![Alpha 3.0 Cosine Match](results/benchmark_charts/twin_cd32_alpha3.0_vs_truth.png)

- **Baseline**
  
  ![Baseline Cosine Match](results/benchmark_charts/twin_cd32_baseline_vs_truth.png)

---

### âœ… Paper-Style Loss Distributions (Train vs Test)

These histograms replicate the visualization style used in the original NEFTune paper. Each pair compares **Baseline** vs NEFTuned model for a specific `Î±`:

- **Î± = 0.5**

  ![Loss Compare 0.5](results/benchmark_charts/loss_compare_alpha_0.5.png)

- **Î± = 0.75**

  ![Loss Compare 0.75](results/benchmark_charts/loss_compare_alpha_0.75.png)

- **Î± = 1.0**

  ![Loss Compare 1.0](results/benchmark_charts/loss_compare_alpha_1.0.png)

- **Î± = 3.0**

  ![Loss Compare 3.0](results/benchmark_charts/loss_compare_alpha_3.0.png)

---

### âœ… All Models vs Ground Truth (Comparison Chart)

One chart showing all model predictions overlaid against ground truth, downsampled for clarity.

![All Model Comparison](results/benchmark_charts/all_models_comparison.png)

> ğŸ“Œ Log-scale histograms show higher training loss for NEFT, but tighter generalization on test set â€” exactly as shown in the NEFTune paper.

---

## ğŸ§  What You Can Learn From This

- How to implement a Siamese embedding compression model
- How to apply NEFTune noise injection for robustness
- How to interpret ranking-based vs value-based metrics
- How to reproduce paper-style visual ablations and histograms
- How to integrate compressed embeddings into **custom RAG pipelines**

---

## ğŸ› ï¸ Run It Yourself

```
# Install requirements
pip install torch datasets sentence-transformers matplotlib

# Run training + loss logging
python train_experiment.py  # or your notebook pipeline

# Generate evaluation reports
python test_all_models.py

# Compare NEFTune variants
python neftune_loss_overlay.py
```

---

## ğŸ“ Credits

- Inspired by [NEFTune: Noisy Embeddings Improve Fine-tuning](https://arxiv.org/abs/2305.14995)
- Embedding backbone: `sentence-transformers/all-MiniLM-L6-v2` (Apache 2.0)
- Developed by Medhavi Monish
